{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Numpy is fundamental for scientific computing in Python. \n",
    "# It provides support for arrays, mathematical functions, random number generation, and more.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# Matplotlib is a plotting library. Here, we import the pyplot interface to provide \n",
    "# easy-to-use functions for creating plots and figures.\n",
    "\n",
    "# import mpl_toolkits.mplot3d.Axes3D\n",
    "# This is commonly used for 3D plotting in Matplotlib. However, it's a part of \n",
    "# Matplotlib and does not need to be explicitly imported unless you are using it directly.\n",
    "\n",
    "from transformers import DPTImageProcessor, DPTForDepthEstimation\n",
    "# Importing from the Hugging Face Transformers library. DPTImageProcessor is used for \n",
    "# preparing images for the DPT model, and DPTForDepthEstimation is the model itself for \n",
    "# estimating depth from images.\n",
    "\n",
    "import torch\n",
    "# PyTorch is a machine learning library. It is used here because the DPT model \n",
    "# is implemented in PyTorch.\n",
    "\n",
    "from PIL import Image\n",
    "# PIL (Python Imaging Library) is used for opening, manipulating, and saving \n",
    "# many different image file formats.\n",
    "\n",
    "import plotly.graph_objs as go\n",
    "# Plotly's graph objects are used for creating and manipulating complex interactive plots.\n",
    "# This will be useful for creating interactive 3D visualizations of the depth maps.\n",
    "\n",
    "# Possible next steps in your script:\n",
    "# - Load and preprocess an image\n",
    "# - Use the DPT model to estimate the depth map from the image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from PIL import Image\n",
    "from transformers import DPTImageProcessor, DPTForDepthEstimation\n",
    "\n",
    "# Load the image\n",
    "# The image is loaded from a specific path within the project workspace.\n",
    "# Replace the path with the correct path to your image file.\n",
    "url = \"Images/170606123840-01-homo-sapiens-fossils.jpg\"\n",
    "image = Image.open(url)\n",
    "\n",
    "# Initialize the Dense Prediction Transformer (DPT) model\n",
    "# DPT is a state-of-the-art model for depth estimation, developed by Intel.\n",
    "# The model 'dpt-large' is used here for its high accuracy in depth estimation.\n",
    "# The model and its corresponding processor are loaded from Hugging Face's transformers library.\n",
    "processor = DPTImageProcessor.from_pretrained(\"Intel/dpt-large\")\n",
    "model = DPTForDepthEstimation.from_pretrained(\"Intel/dpt-large\")\n",
    "\n",
    "# Note: Ensure that the Intel/dpt-large model is available in your environment.\n",
    "# You may need to download and cache the model prior to running this code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the input image for the DPT model\n",
    "# The processor reformats the image (resizing, normalizing, etc.) to be compatible with the model's input requirements.\n",
    "inputs = processor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "# Model inference\n",
    "# Using a no_grad() context for inference because gradients are not needed for model evaluation.\n",
    "# This reduces memory consumption and speeds up computations.\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    predicted_depth = outputs.predicted_depth\n",
    "\n",
    "# Interpolate the predicted depth to match the original image size\n",
    "# This is necessary because the depth estimation model often changes the resolution of the input.\n",
    "prediction = torch.nn.functional.interpolate(\n",
    "    predicted_depth.unsqueeze(1),\n",
    "    size=image.size[::-1],  # Resizing to the original image size\n",
    "    mode=\"bicubic\",         # Bicubic interpolation for resizing\n",
    "    align_corners=False\n",
    ")\n",
    "\n",
    "# Convert the prediction to a numpy array and normalize\n",
    "# The depth map is converted from a PyTorch tensor to a numpy array for easier manipulation and visualization.\n",
    "# Normalization is done to scale the depth values to a range that is suitable for visualization (0-255).\n",
    "depth_map = prediction.squeeze().cpu().numpy()\n",
    "depth_map = (depth_map * 255 / np.max(depth_map)).astype(\"uint8\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the original PIL image to a NumPy array\n",
    "# This step is necessary to extract color information for each pixel in the image.\n",
    "image_np = np.array(image)\n",
    "\n",
    "# Extracting the dimensions of the depth map\n",
    "# 'height' and 'width' will be used to iterate over the depth map and to calculate 3D coordinates.\n",
    "height, width = depth_map.shape\n",
    "\n",
    "# Camera parameters\n",
    "# 'fx' and 'fy' represent the focal lengths of the camera in the horizontal and vertical directions.\n",
    "# These are used to convert 2D pixel coordinates to 3D world coordinates.\n",
    "fx, fy = 525.0, 525.0  # Example values, should be set according to your camera specs\n",
    "\n",
    "# 'cx' and 'cy' represent the optical center of the image, typically at the center of the image.\n",
    "cx, cy = width / 2, height / 2\n",
    "\n",
    "# Initialize lists to store 3D points and their corresponding colors\n",
    "points = []\n",
    "colors = []\n",
    "\n",
    "# Next steps (not shown in this snippet):\n",
    "# - Iterate over each pixel in the depth map.\n",
    "# - Use the depth value and camera parameters to calculate the 3D coordinates for each pixel.\n",
    "# - Extract the color of each pixel from the original image and associate it with the corresponding 3D point.\n",
    "# - Append each 3D point and its color to the 'points' and 'colors' lists, respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over each pixel in the depth map\n",
    "for v in range(height):\n",
    "    for u in range(width):\n",
    "        # Get the depth value from the depth map\n",
    "        Z = depth_map[v, u]\n",
    "\n",
    "        # Skip processing if depth value is zero (indicating no data or background)\n",
    "        if Z == 0: continue\n",
    "\n",
    "        # Convert depth map coordinates to 3D coordinates\n",
    "        # X and Y coordinates are calculated using the intrinsic camera parameters\n",
    "        # (u, v) are the pixel coordinates, (cx, cy) is the optical center, and fx, fy are the focal lengths\n",
    "        X = (u - cx) * Z / fx\n",
    "        Y = (v - cy) * Z / fy\n",
    "\n",
    "        # Append the 3D coordinates to the points list\n",
    "        points.append([X, Y, Z])\n",
    "\n",
    "        # Get the corresponding color from the original image and append it to the colors list\n",
    "        # The color is formatted as an RGB string for Plotly\n",
    "        colors.append('rgb({},{},{})'.format(*image_np[v, u]))\n",
    "\n",
    "# Convert the list of points to a NumPy array for easier manipulation\n",
    "points = np.array(points)\n",
    "\n",
    "# Create a Plotly 3D scatter plot trace\n",
    "# x, y, and z are the coordinates for each point\n",
    "# The color of each point is set to the corresponding color extracted from the original image\n",
    "trace = go.Scatter3d(\n",
    "    x=points[:, 0],\n",
    "    y=points[:, 1],\n",
    "    z=points[:, 2],\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        size=1,         # Size of the markers\n",
    "        color=colors,   # Colors of the markers\n",
    "    )\n",
    ")\n",
    "\n",
    "# Assemble the plot data\n",
    "data = [trace]\n",
    "\n",
    "# Define the layout for the 3D plot\n",
    "layout = go.Layout(\n",
    "    margin=dict(l=0, r=0, b=0, t=0)  # Minimal margins for the plot\n",
    ")\n",
    "\n",
    "# Create a figure with the data and layout\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "\n",
    "# Save the figure as an HTML file for interactive viewing\n",
    "fig.write_html(\"3d_plot2.html\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Convert the PIL image to a NumPy array for color extraction\n",
    "image_np = np.array(image)\n",
    "\n",
    "# Extract the dimensions of the depth map\n",
    "height, width = depth_map.shape\n",
    "\n",
    "# Define the camera parameters\n",
    "fx, fy = 525.0, 525.0  # Focal lengths (assumed values; replace with actual values if known)\n",
    "cx, cy = width / 2, height / 2  # Optical center (assumed to be at the center of the image)\n",
    "\n",
    "# Initialize lists for storing 3D points and their corresponding colors\n",
    "points = []\n",
    "colors = []\n",
    "\n",
    "# Iterate over each pixel in the depth map\n",
    "for v in range(height):\n",
    "    for u in range(width):\n",
    "        Z = depth_map[v, u]  # Depth value\n",
    "        if Z == 0: continue  # Skip points with zero depth\n",
    "\n",
    "        # Convert pixel coordinates (u, v) and depth (Z) to 3D coordinates (X, Y, Z)\n",
    "        X = (u - cx) * Z / fx\n",
    "        Y = (v - cy) * Z / fy\n",
    "        points.append([X, Y, Z])  # Add the 3D point to the points list\n",
    "\n",
    "        # Normalize the color values and add to the colors list\n",
    "        colors.append(image_np[v, u] / 255)  # Assuming image_np is in RGB format\n",
    "\n",
    "# Convert the lists to NumPy arrays for easier manipulation and plotting\n",
    "points = np.array(points)\n",
    "colors = np.array(colors)\n",
    "\n",
    "# Create a 3D scatter plot using Matplotlib\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Plot all points. The color of each point is determined by the corresponding pixel color in the image\n",
    "ax.scatter(points[:, 0], points[:, 1], points[:, 2], c=colors, marker='.')\n",
    "\n",
    "# Label the axes\n",
    "ax.set_xlabel('X Axis')\n",
    "ax.set_ylabel('Y Axis')\n",
    "ax.set_zlabel('Z Axis')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
